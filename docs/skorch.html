---

title: Testing skorch


keywords: fastai
sidebar: home_sidebar

summary: "Why py explored?"
description: "Why py explored?"
nb_path: "06_skorch.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 06_skorch.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">from</span> <span class="nn">skorch</span> <span class="kn">import</span> <span class="n">NeuralNetClassifier</span>


<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">nonlin</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dense0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">num_units</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nonlin</span> <span class="o">=</span> <span class="n">nonlin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_units</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nonlin</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dense0</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">X</span>


<span class="n">net</span> <span class="o">=</span> <span class="n">NeuralNetClassifier</span><span class="p">(</span>
    <span class="n">MyModule</span><span class="p">,</span>
    <span class="n">max_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="c1"># Shuffle training data on each epoch</span>
    <span class="n">iterator_train__shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">net</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_proba</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7018</span>       <span class="ansi-green-fg">0.6100</span>        <span class="ansi-magenta-fg">0.6777</span>  0.0416
      2        <span class="ansi-cyan-fg">0.6895</span>       <span class="ansi-green-fg">0.6600</span>        <span class="ansi-magenta-fg">0.6645</span>  0.0454
      3        <span class="ansi-cyan-fg">0.6637</span>       <span class="ansi-green-fg">0.6800</span>        <span class="ansi-magenta-fg">0.6526</span>  0.0390
      4        <span class="ansi-cyan-fg">0.6624</span>       <span class="ansi-green-fg">0.7000</span>        <span class="ansi-magenta-fg">0.6418</span>  0.0380
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
/home/danph/anaconda3/envs/dev/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)
  Variable._execution_engine.run_backward(
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      5        <span class="ansi-cyan-fg">0.6438</span>       <span class="ansi-green-fg">0.7200</span>        <span class="ansi-magenta-fg">0.6291</span>  0.0392
      6        <span class="ansi-cyan-fg">0.6299</span>       0.7200        <span class="ansi-magenta-fg">0.6195</span>  0.0395
      7        <span class="ansi-cyan-fg">0.6256</span>       0.7200        <span class="ansi-magenta-fg">0.6058</span>  0.0362
      8        <span class="ansi-cyan-fg">0.6072</span>       <span class="ansi-green-fg">0.7350</span>        <span class="ansi-magenta-fg">0.5945</span>  0.0419
      9        0.6084       0.7250        <span class="ansi-magenta-fg">0.5890</span>  0.0389
     10        <span class="ansi-cyan-fg">0.5937</span>       <span class="ansi-green-fg">0.7400</span>        <span class="ansi-magenta-fg">0.5765</span>  0.0381
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">],</span>
    <span class="s1">&#39;max_epochs&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
    <span class="s1">&#39;module__num_units&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
<span class="p">}</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>

<span class="n">gs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gs</span><span class="o">.</span><span class="n">best_score_</span><span class="p">,</span> <span class="n">gs</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7338</span>       <span class="ansi-green-fg">0.5000</span>        <span class="ansi-magenta-fg">0.7109</span>  0.0165
      2        <span class="ansi-cyan-fg">0.7303</span>       0.5000        <span class="ansi-magenta-fg">0.7089</span>  0.0298
      3        0.7305       0.5000        <span class="ansi-magenta-fg">0.7072</span>  0.0255
      4        <span class="ansi-cyan-fg">0.7279</span>       0.5000        <span class="ansi-magenta-fg">0.7059</span>  0.0232
      5        <span class="ansi-cyan-fg">0.7216</span>       0.5000        <span class="ansi-magenta-fg">0.7046</span>  0.0250
      6        0.7242       <span class="ansi-green-fg">0.5075</span>        <span class="ansi-magenta-fg">0.7030</span>  0.0221
      7        <span class="ansi-cyan-fg">0.7189</span>       0.5075        <span class="ansi-magenta-fg">0.7020</span>  0.0172
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      8        0.7193       0.5075        <span class="ansi-magenta-fg">0.7011</span>  0.0298
      9        <span class="ansi-cyan-fg">0.7142</span>       0.5075        <span class="ansi-magenta-fg">0.6999</span>  0.0321
     10        <span class="ansi-cyan-fg">0.7094</span>       0.5075        <span class="ansi-magenta-fg">0.6987</span>  0.0230
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7099</span>       <span class="ansi-green-fg">0.4776</span>        <span class="ansi-magenta-fg">0.6918</span>  0.0216
      2        <span class="ansi-cyan-fg">0.7055</span>       <span class="ansi-green-fg">0.4925</span>        <span class="ansi-magenta-fg">0.6903</span>  0.0223
      3        <span class="ansi-cyan-fg">0.7023</span>       <span class="ansi-green-fg">0.5000</span>        <span class="ansi-magenta-fg">0.6887</span>  0.0253
      4        0.7145       0.4925        <span class="ansi-magenta-fg">0.6883</span>  0.0242
      5        <span class="ansi-cyan-fg">0.7012</span>       <span class="ansi-green-fg">0.5149</span>        <span class="ansi-magenta-fg">0.6870</span>  0.0170
      6        <span class="ansi-cyan-fg">0.6991</span>       <span class="ansi-green-fg">0.5299</span>        <span class="ansi-magenta-fg">0.6858</span>  0.0303
      7        <span class="ansi-cyan-fg">0.6935</span>       0.5299        <span class="ansi-magenta-fg">0.6852</span>  0.0281
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      8        0.6980       <span class="ansi-green-fg">0.5448</span>        <span class="ansi-magenta-fg">0.6844</span>  0.0158
      9        0.6952       0.5373        <span class="ansi-magenta-fg">0.6837</span>  0.0201
     10        0.6971       0.5373        <span class="ansi-magenta-fg">0.6830</span>  0.0246
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7373</span>       <span class="ansi-green-fg">0.4851</span>        <span class="ansi-magenta-fg">0.7197</span>  0.0230
      2        <span class="ansi-cyan-fg">0.7243</span>       0.4851        <span class="ansi-magenta-fg">0.7170</span>  0.0254
      3        0.7267       0.4776        <span class="ansi-magenta-fg">0.7150</span>  0.0254
      4        <span class="ansi-cyan-fg">0.7153</span>       0.4851        <span class="ansi-magenta-fg">0.7131</span>  0.0230
      5        <span class="ansi-cyan-fg">0.7086</span>       0.4851        <span class="ansi-magenta-fg">0.7113</span>  0.0229
      6        0.7123       0.4851        <span class="ansi-magenta-fg">0.7094</span>  0.0243
      7        0.7110       0.4776        <span class="ansi-magenta-fg">0.7080</span>  0.0203
      8        <span class="ansi-cyan-fg">0.7048</span>       0.4851        <span class="ansi-magenta-fg">0.7063</span>  0.0170
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      9        <span class="ansi-cyan-fg">0.7019</span>       0.4776        <span class="ansi-magenta-fg">0.7051</span>  0.0199
     10        0.7027       0.4701        <span class="ansi-magenta-fg">0.7036</span>  0.0455
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.6879</span>       <span class="ansi-green-fg">0.6716</span>        <span class="ansi-magenta-fg">0.6694</span>  0.0217
      2        0.6883       0.6716        <span class="ansi-magenta-fg">0.6682</span>  0.0273
      3        <span class="ansi-cyan-fg">0.6878</span>       0.6642        <span class="ansi-magenta-fg">0.6671</span>  0.0199
      4        <span class="ansi-cyan-fg">0.6866</span>       0.6716        <span class="ansi-magenta-fg">0.6656</span>  0.0282
      5        <span class="ansi-cyan-fg">0.6723</span>       <span class="ansi-green-fg">0.6866</span>        <span class="ansi-magenta-fg">0.6645</span>  0.0238
      6        0.6795       <span class="ansi-green-fg">0.6940</span>        <span class="ansi-magenta-fg">0.6634</span>  0.0168
      7        0.6840       <span class="ansi-green-fg">0.7090</span>        <span class="ansi-magenta-fg">0.6627</span>  0.0218
      8        0.6755       <span class="ansi-green-fg">0.7239</span>        <span class="ansi-magenta-fg">0.6623</span>  0.0236
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      9        0.6817       0.7239        <span class="ansi-magenta-fg">0.6615</span>  0.0217
     10        0.6785       0.7239        <span class="ansi-magenta-fg">0.6608</span>  0.0162
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7018</span>       <span class="ansi-green-fg">0.5522</span>        <span class="ansi-magenta-fg">0.6879</span>  0.0232
      2        0.7023       0.5448        <span class="ansi-magenta-fg">0.6866</span>  0.0230
      3        0.7059       0.5448        <span class="ansi-magenta-fg">0.6853</span>  0.0298
      4        <span class="ansi-cyan-fg">0.6880</span>       <span class="ansi-green-fg">0.5597</span>        <span class="ansi-magenta-fg">0.6843</span>  0.0240
      5        <span class="ansi-cyan-fg">0.6858</span>       <span class="ansi-green-fg">0.5672</span>        <span class="ansi-magenta-fg">0.6833</span>  0.0230
      6        0.6950       0.5672        <span class="ansi-magenta-fg">0.6823</span>  0.0251
      7        0.6884       <span class="ansi-green-fg">0.5821</span>        <span class="ansi-magenta-fg">0.6815</span>  0.0235
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      8        0.6932       0.5746        <span class="ansi-magenta-fg">0.6808</span>  0.0278
      9        0.6897       0.5746        <span class="ansi-magenta-fg">0.6801</span>  0.0210
     10        <span class="ansi-cyan-fg">0.6844</span>       0.5821        <span class="ansi-magenta-fg">0.6794</span>  0.0261
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7152</span>       <span class="ansi-green-fg">0.5000</span>        <span class="ansi-magenta-fg">0.7070</span>  0.0276
      2        <span class="ansi-cyan-fg">0.7145</span>       0.5000        <span class="ansi-magenta-fg">0.7037</span>  0.0242
      3        <span class="ansi-cyan-fg">0.7120</span>       0.5000        <span class="ansi-magenta-fg">0.7002</span>  0.0264
      4        0.7125       0.5000        <span class="ansi-magenta-fg">0.6972</span>  0.0230
      5        <span class="ansi-cyan-fg">0.6962</span>       0.5000        <span class="ansi-magenta-fg">0.6938</span>  0.0248
      6        0.6971       0.5000        <span class="ansi-magenta-fg">0.6909</span>  0.0353
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      7        <span class="ansi-cyan-fg">0.6895</span>       0.5000        <span class="ansi-magenta-fg">0.6881</span>  0.0224
      8        0.6917       0.5000        <span class="ansi-magenta-fg">0.6859</span>  0.0288
      9        <span class="ansi-cyan-fg">0.6849</span>       0.4925        <span class="ansi-magenta-fg">0.6841</span>  0.0281
     10        <span class="ansi-cyan-fg">0.6830</span>       0.4925        <span class="ansi-magenta-fg">0.6822</span>  0.0240
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7070</span>       <span class="ansi-green-fg">0.5373</span>        <span class="ansi-magenta-fg">0.7086</span>  0.0249
      2        0.7193       <span class="ansi-green-fg">0.5746</span>        <span class="ansi-magenta-fg">0.7056</span>  0.0251
      3        0.7157       0.5522        <span class="ansi-magenta-fg">0.7026</span>  0.0181
      4        0.7130       0.5522        <span class="ansi-magenta-fg">0.7017</span>  0.0275
      5        <span class="ansi-cyan-fg">0.7047</span>       0.5597        <span class="ansi-magenta-fg">0.7001</span>  0.0194
      6        0.7095       0.5373        <span class="ansi-magenta-fg">0.6982</span>  0.0272
      7        <span class="ansi-cyan-fg">0.7040</span>       0.5373        <span class="ansi-magenta-fg">0.6965</span>  0.0206
      8        <span class="ansi-cyan-fg">0.7006</span>       0.5373        <span class="ansi-magenta-fg">0.6952</span>  0.0172
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      9        <span class="ansi-cyan-fg">0.6967</span>       0.5448        <span class="ansi-magenta-fg">0.6942</span>  0.0274
     10        0.7064       0.5373        <span class="ansi-magenta-fg">0.6926</span>  0.0234
     11        0.7001       0.5522        <span class="ansi-magenta-fg">0.6917</span>  0.0212
     12        0.6980       0.5522        <span class="ansi-magenta-fg">0.6909</span>  0.0176
     13        <span class="ansi-cyan-fg">0.6939</span>       0.5522        <span class="ansi-magenta-fg">0.6903</span>  0.0178
     14        <span class="ansi-cyan-fg">0.6896</span>       0.5522        <span class="ansi-magenta-fg">0.6895</span>  0.0216
     15        0.7024       0.5522        <span class="ansi-magenta-fg">0.6889</span>  0.0197
     16        0.6961       0.5522        <span class="ansi-magenta-fg">0.6885</span>  0.0155
     17        0.6945       0.5522        <span class="ansi-magenta-fg">0.6880</span>  0.0177
     18        0.6968       0.5299        <span class="ansi-magenta-fg">0.6873</span>  0.0243
     19        <span class="ansi-cyan-fg">0.6827</span>       0.5299        <span class="ansi-magenta-fg">0.6867</span>  0.0205
     20        0.7008       0.5448        <span class="ansi-magenta-fg">0.6863</span>  0.0203
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7167</span>       <span class="ansi-green-fg">0.4925</span>        <span class="ansi-magenta-fg">0.7108</span>  0.0254
      2        <span class="ansi-cyan-fg">0.7071</span>       0.4925        <span class="ansi-magenta-fg">0.7087</span>  0.0174
      3        0.7181       0.4925        <span class="ansi-magenta-fg">0.7067</span>  0.0224
      4        <span class="ansi-cyan-fg">0.7066</span>       0.4851        <span class="ansi-magenta-fg">0.7052</span>  0.0218
      5        0.7071       0.4851        <span class="ansi-magenta-fg">0.7042</span>  0.0216
      6        <span class="ansi-cyan-fg">0.7040</span>       0.4851        <span class="ansi-magenta-fg">0.7027</span>  0.0221
      7        0.7062       0.4851        <span class="ansi-magenta-fg">0.7019</span>  0.0198
      8        <span class="ansi-cyan-fg">0.6983</span>       <span class="ansi-green-fg">0.5000</span>        <span class="ansi-magenta-fg">0.7008</span>  0.0222
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      9        0.7025       <span class="ansi-green-fg">0.5075</span>        <span class="ansi-magenta-fg">0.6997</span>  0.0234
     10        <span class="ansi-cyan-fg">0.6934</span>       <span class="ansi-green-fg">0.5224</span>        <span class="ansi-magenta-fg">0.6992</span>  0.0283
     11        0.6953       0.5149        <span class="ansi-magenta-fg">0.6983</span>  0.0258
     12        0.6951       <span class="ansi-green-fg">0.5299</span>        <span class="ansi-magenta-fg">0.6976</span>  0.0214
     13        <span class="ansi-cyan-fg">0.6925</span>       <span class="ansi-green-fg">0.5373</span>        <span class="ansi-magenta-fg">0.6971</span>  0.0164
     14        0.6939       <span class="ansi-green-fg">0.5448</span>        <span class="ansi-magenta-fg">0.6964</span>  0.0250
     15        0.6962       <span class="ansi-green-fg">0.5522</span>        <span class="ansi-magenta-fg">0.6958</span>  0.0232
     16        0.6954       0.5299        <span class="ansi-magenta-fg">0.6952</span>  0.0208
     17        <span class="ansi-cyan-fg">0.6914</span>       0.5149        <span class="ansi-magenta-fg">0.6946</span>  0.0206
     18        <span class="ansi-cyan-fg">0.6874</span>       0.5149        <span class="ansi-magenta-fg">0.6938</span>  0.0283
     19        <span class="ansi-cyan-fg">0.6866</span>       0.5299        <span class="ansi-magenta-fg">0.6934</span>  0.0167
     20        <span class="ansi-cyan-fg">0.6847</span>       0.5448        <span class="ansi-magenta-fg">0.6927</span>  0.0179
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.6814</span>       <span class="ansi-green-fg">0.5896</span>        <span class="ansi-magenta-fg">0.6818</span>  0.0189
      2        <span class="ansi-cyan-fg">0.6770</span>       0.5896        <span class="ansi-magenta-fg">0.6791</span>  0.0222
      3        0.6793       0.5896        <span class="ansi-magenta-fg">0.6769</span>  0.0242
      4        0.6821       0.5821        <span class="ansi-magenta-fg">0.6756</span>  0.0201
      5        0.6825       <span class="ansi-green-fg">0.5970</span>        <span class="ansi-magenta-fg">0.6742</span>  0.0158
      6        <span class="ansi-cyan-fg">0.6757</span>       0.5821        <span class="ansi-magenta-fg">0.6725</span>  0.0259
      7        <span class="ansi-cyan-fg">0.6679</span>       0.5896        <span class="ansi-magenta-fg">0.6711</span>  0.0226
      8        0.6726       0.5970        <span class="ansi-magenta-fg">0.6695</span>  0.0207
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      9        0.6701       0.5970        <span class="ansi-magenta-fg">0.6681</span>  0.0252
     10        <span class="ansi-cyan-fg">0.6631</span>       <span class="ansi-green-fg">0.6045</span>        <span class="ansi-magenta-fg">0.6666</span>  0.0213
     11        <span class="ansi-cyan-fg">0.6614</span>       0.6045        <span class="ansi-magenta-fg">0.6654</span>  0.0208
     12        0.6636       <span class="ansi-green-fg">0.6119</span>        <span class="ansi-magenta-fg">0.6650</span>  0.0296
     13        0.6730       0.6119        <span class="ansi-magenta-fg">0.6641</span>  0.0214
     14        0.6622       0.6119        <span class="ansi-magenta-fg">0.6629</span>  0.0265
     15        <span class="ansi-cyan-fg">0.6592</span>       0.6119        <span class="ansi-magenta-fg">0.6618</span>  0.0182
     16        0.6620       <span class="ansi-green-fg">0.6194</span>        <span class="ansi-magenta-fg">0.6612</span>  0.0170
     17        <span class="ansi-cyan-fg">0.6502</span>       0.6119        <span class="ansi-magenta-fg">0.6602</span>  0.0221
     18        0.6557       0.6194        <span class="ansi-magenta-fg">0.6597</span>  0.0220
     19        0.6531       <span class="ansi-green-fg">0.6269</span>        <span class="ansi-magenta-fg">0.6593</span>  0.0264
     20        0.6583       0.6269        <span class="ansi-magenta-fg">0.6587</span>  0.0218
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.6935</span>       <span class="ansi-green-fg">0.5075</span>        <span class="ansi-magenta-fg">0.6962</span>  0.0253
      2        <span class="ansi-cyan-fg">0.6890</span>       0.5075        <span class="ansi-magenta-fg">0.6936</span>  0.0267
      3        <span class="ansi-cyan-fg">0.6790</span>       <span class="ansi-green-fg">0.5299</span>        <span class="ansi-magenta-fg">0.6907</span>  0.0235
      4        0.6894       <span class="ansi-green-fg">0.5373</span>        <span class="ansi-magenta-fg">0.6876</span>  0.0214
      5        <span class="ansi-cyan-fg">0.6679</span>       0.5299        <span class="ansi-magenta-fg">0.6845</span>  0.0234
      6        0.6768       <span class="ansi-green-fg">0.5522</span>        <span class="ansi-magenta-fg">0.6827</span>  0.0228
      7        0.6688       0.5448        <span class="ansi-magenta-fg">0.6818</span>  0.0165
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      8        <span class="ansi-cyan-fg">0.6620</span>       <span class="ansi-green-fg">0.5597</span>        <span class="ansi-magenta-fg">0.6801</span>  0.0259
      9        0.6663       0.5597        <span class="ansi-magenta-fg">0.6784</span>  0.0172
     10        0.6661       0.5597        <span class="ansi-magenta-fg">0.6768</span>  0.0210
     11        0.6643       <span class="ansi-green-fg">0.5821</span>        <span class="ansi-magenta-fg">0.6752</span>  0.0232
     12        0.6696       0.5821        <span class="ansi-magenta-fg">0.6728</span>  0.0226
     13        <span class="ansi-cyan-fg">0.6576</span>       0.5821        <span class="ansi-magenta-fg">0.6704</span>  0.0272
     14        <span class="ansi-cyan-fg">0.6520</span>       0.5746        <span class="ansi-magenta-fg">0.6680</span>  0.0223
     15        0.6589       <span class="ansi-green-fg">0.6045</span>        <span class="ansi-magenta-fg">0.6657</span>  0.0198
     16        0.6522       0.6045        <span class="ansi-magenta-fg">0.6645</span>  0.0205
     17        <span class="ansi-cyan-fg">0.6435</span>       0.5970        <span class="ansi-magenta-fg">0.6627</span>  0.0240
     18        <span class="ansi-cyan-fg">0.6413</span>       <span class="ansi-green-fg">0.6119</span>        <span class="ansi-magenta-fg">0.6608</span>  0.0191
     19        0.6415       0.6045        <span class="ansi-magenta-fg">0.6593</span>  0.0183
     20        <span class="ansi-cyan-fg">0.6369</span>       <span class="ansi-green-fg">0.6194</span>        <span class="ansi-magenta-fg">0.6575</span>  0.0162
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7021</span>       <span class="ansi-green-fg">0.5000</span>        <span class="ansi-magenta-fg">0.6998</span>  0.0191
      2        0.7036       0.5000        <span class="ansi-magenta-fg">0.6983</span>  0.0183
      3        <span class="ansi-cyan-fg">0.6921</span>       0.5000        <span class="ansi-magenta-fg">0.6964</span>  0.0293
      4        <span class="ansi-cyan-fg">0.6902</span>       0.5000        <span class="ansi-magenta-fg">0.6951</span>  0.0185
      5        0.6926       0.5000        <span class="ansi-magenta-fg">0.6939</span>  0.0272
      6        <span class="ansi-cyan-fg">0.6884</span>       0.5000        <span class="ansi-magenta-fg">0.6920</span>  0.0199
      7        0.6921       0.5000        <span class="ansi-magenta-fg">0.6907</span>  0.0295
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      8        <span class="ansi-cyan-fg">0.6757</span>       0.5000        <span class="ansi-magenta-fg">0.6893</span>  0.0290
      9        0.6893       0.5000        <span class="ansi-magenta-fg">0.6880</span>  0.0266
     10        0.6915       0.5000        <span class="ansi-magenta-fg">0.6873</span>  0.0223
     11        0.6898       0.5000        <span class="ansi-magenta-fg">0.6861</span>  0.0229
     12        0.6859       0.5000        <span class="ansi-magenta-fg">0.6852</span>  0.0205
     13        0.6855       0.5000        <span class="ansi-magenta-fg">0.6844</span>  0.0226
     14        0.6762       0.5000        <span class="ansi-magenta-fg">0.6834</span>  0.0180
     15        <span class="ansi-cyan-fg">0.6716</span>       0.5000        <span class="ansi-magenta-fg">0.6824</span>  0.0267
     16        0.6800       0.5000        <span class="ansi-magenta-fg">0.6817</span>  0.0176
     17        0.6763       0.5000        <span class="ansi-magenta-fg">0.6810</span>  0.0232
     18        0.6755       0.5000        <span class="ansi-magenta-fg">0.6799</span>  0.0247
     19        <span class="ansi-cyan-fg">0.6708</span>       0.5000        <span class="ansi-magenta-fg">0.6791</span>  0.0204
     20        0.6737       <span class="ansi-green-fg">0.5075</span>        <span class="ansi-magenta-fg">0.6781</span>  0.0170
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.6966</span>       <span class="ansi-green-fg">0.4925</span>        <span class="ansi-magenta-fg">0.6868</span>  0.0185
      2        0.7005       0.4851        <span class="ansi-magenta-fg">0.6857</span>  0.0170
      3        <span class="ansi-cyan-fg">0.6839</span>       <span class="ansi-green-fg">0.5224</span>        <span class="ansi-magenta-fg">0.6848</span>  0.0170
      4        0.6894       <span class="ansi-green-fg">0.5448</span>        <span class="ansi-magenta-fg">0.6842</span>  0.0155
      5        0.6851       <span class="ansi-green-fg">0.5597</span>        <span class="ansi-magenta-fg">0.6833</span>  0.0204
      6        <span class="ansi-cyan-fg">0.6800</span>       <span class="ansi-green-fg">0.5672</span>        <span class="ansi-magenta-fg">0.6826</span>  0.0197
      7        0.6906       <span class="ansi-green-fg">0.5896</span>        <span class="ansi-magenta-fg">0.6818</span>  0.0167
      8        0.6838       <span class="ansi-green-fg">0.6194</span>        <span class="ansi-magenta-fg">0.6814</span>  0.0245
      9        0.6811       0.6194        <span class="ansi-magenta-fg">0.6807</span>  0.0228
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>     10        <span class="ansi-cyan-fg">0.6774</span>       <span class="ansi-green-fg">0.6269</span>        <span class="ansi-magenta-fg">0.6803</span>  0.0220
     11        <span class="ansi-cyan-fg">0.6743</span>       0.6119        <span class="ansi-magenta-fg">0.6800</span>  0.0234
     12        0.6812       0.6269        <span class="ansi-magenta-fg">0.6795</span>  0.0207
     13        <span class="ansi-cyan-fg">0.6725</span>       0.6269        <span class="ansi-magenta-fg">0.6790</span>  0.0222
     14        0.6761       0.6194        <span class="ansi-magenta-fg">0.6785</span>  0.0244
     15        0.6764       0.6045        <span class="ansi-magenta-fg">0.6780</span>  0.0227
     16        <span class="ansi-cyan-fg">0.6692</span>       0.6045        <span class="ansi-magenta-fg">0.6777</span>  0.0266
     17        <span class="ansi-cyan-fg">0.6670</span>       0.5970        <span class="ansi-magenta-fg">0.6772</span>  0.0210
     18        0.6685       0.6045        <span class="ansi-magenta-fg">0.6767</span>  0.0276
     19        0.6678       0.6119        <span class="ansi-magenta-fg">0.6763</span>  0.0257
     20        0.6753       0.6045        <span class="ansi-magenta-fg">0.6759</span>  0.0241
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7581</span>       <span class="ansi-green-fg">0.3806</span>        <span class="ansi-magenta-fg">0.7342</span>  0.0237
      2        <span class="ansi-cyan-fg">0.7437</span>       <span class="ansi-green-fg">0.4179</span>        <span class="ansi-magenta-fg">0.7186</span>  0.0192
      3        <span class="ansi-cyan-fg">0.7175</span>       <span class="ansi-green-fg">0.4254</span>        <span class="ansi-magenta-fg">0.7106</span>  0.0214
      4        <span class="ansi-cyan-fg">0.7079</span>       <span class="ansi-green-fg">0.4328</span>        <span class="ansi-magenta-fg">0.7043</span>  0.0193
      5        0.7117       <span class="ansi-green-fg">0.4552</span>        <span class="ansi-magenta-fg">0.6968</span>  0.0246
      6        <span class="ansi-cyan-fg">0.7033</span>       <span class="ansi-green-fg">0.4851</span>        <span class="ansi-magenta-fg">0.6913</span>  0.0215
      7        <span class="ansi-cyan-fg">0.6997</span>       <span class="ansi-green-fg">0.5000</span>        <span class="ansi-magenta-fg">0.6871</span>  0.0297
      8        <span class="ansi-cyan-fg">0.6869</span>       <span class="ansi-green-fg">0.5149</span>        <span class="ansi-magenta-fg">0.6826</span>  0.0210
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      9        0.6971       <span class="ansi-green-fg">0.5224</span>        <span class="ansi-magenta-fg">0.6794</span>  0.0196
     10        <span class="ansi-cyan-fg">0.6834</span>       0.5224        <span class="ansi-magenta-fg">0.6752</span>  0.0227
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7006</span>       <span class="ansi-green-fg">0.5821</span>        <span class="ansi-magenta-fg">0.6885</span>  0.0148
      2        <span class="ansi-cyan-fg">0.6976</span>       0.5821        <span class="ansi-magenta-fg">0.6867</span>  0.0243
      3        <span class="ansi-cyan-fg">0.6927</span>       <span class="ansi-green-fg">0.5896</span>        <span class="ansi-magenta-fg">0.6846</span>  0.0239
      4        <span class="ansi-cyan-fg">0.6875</span>       <span class="ansi-green-fg">0.6045</span>        <span class="ansi-magenta-fg">0.6831</span>  0.0191
      5        0.6885       0.6045        <span class="ansi-magenta-fg">0.6819</span>  0.0270
      6        0.6923       0.6045        <span class="ansi-magenta-fg">0.6808</span>  0.0220
      7        <span class="ansi-cyan-fg">0.6836</span>       0.5821        <span class="ansi-magenta-fg">0.6792</span>  0.0279
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      8        <span class="ansi-cyan-fg">0.6738</span>       0.5821        <span class="ansi-magenta-fg">0.6771</span>  0.0282
      9        <span class="ansi-cyan-fg">0.6732</span>       0.5896        <span class="ansi-magenta-fg">0.6762</span>  0.0195
     10        0.6830       0.5896        <span class="ansi-magenta-fg">0.6749</span>  0.0242
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.6918</span>       <span class="ansi-green-fg">0.5597</span>        <span class="ansi-magenta-fg">0.6830</span>  0.0211
      2        <span class="ansi-cyan-fg">0.6890</span>       <span class="ansi-green-fg">0.5746</span>        <span class="ansi-magenta-fg">0.6817</span>  0.0267
      3        <span class="ansi-cyan-fg">0.6888</span>       0.5746        <span class="ansi-magenta-fg">0.6810</span>  0.0175
      4        <span class="ansi-cyan-fg">0.6861</span>       <span class="ansi-green-fg">0.5821</span>        <span class="ansi-magenta-fg">0.6801</span>  0.0228
      5        <span class="ansi-cyan-fg">0.6756</span>       <span class="ansi-green-fg">0.6119</span>        <span class="ansi-magenta-fg">0.6792</span>  0.0479
      6        0.6823       0.5970        <span class="ansi-magenta-fg">0.6782</span>  0.0240
      7        <span class="ansi-cyan-fg">0.6755</span>       <span class="ansi-green-fg">0.6343</span>        <span class="ansi-magenta-fg">0.6778</span>  0.0184
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      8        0.6761       0.6045        <span class="ansi-magenta-fg">0.6774</span>  0.0254
      9        0.6755       <span class="ansi-green-fg">0.6493</span>        <span class="ansi-magenta-fg">0.6768</span>  0.0163
     10        <span class="ansi-cyan-fg">0.6748</span>       0.6493        <span class="ansi-magenta-fg">0.6756</span>  0.0197
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.6955</span>       <span class="ansi-green-fg">0.5821</span>        <span class="ansi-magenta-fg">0.6840</span>  0.0127
      2        <span class="ansi-cyan-fg">0.6921</span>       0.5821        <span class="ansi-magenta-fg">0.6809</span>  0.0217
      3        <span class="ansi-cyan-fg">0.6773</span>       <span class="ansi-green-fg">0.5970</span>        <span class="ansi-magenta-fg">0.6788</span>  0.0203
      4        0.6832       0.5970        <span class="ansi-magenta-fg">0.6756</span>  0.0223
      5        0.6778       <span class="ansi-green-fg">0.6045</span>        <span class="ansi-magenta-fg">0.6734</span>  0.0196
      6        <span class="ansi-cyan-fg">0.6734</span>       <span class="ansi-green-fg">0.6269</span>        <span class="ansi-magenta-fg">0.6701</span>  0.0161
      7        0.6768       <span class="ansi-green-fg">0.6418</span>        <span class="ansi-magenta-fg">0.6668</span>  0.0222
      8        <span class="ansi-cyan-fg">0.6714</span>       <span class="ansi-green-fg">0.6567</span>        <span class="ansi-magenta-fg">0.6627</span>  0.0215
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      9        <span class="ansi-cyan-fg">0.6568</span>       <span class="ansi-green-fg">0.6642</span>        <span class="ansi-magenta-fg">0.6596</span>  0.0278
     10        0.6631       <span class="ansi-green-fg">0.6716</span>        <span class="ansi-magenta-fg">0.6574</span>  0.0251
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.6919</span>       <span class="ansi-green-fg">0.6119</span>        <span class="ansi-magenta-fg">0.7003</span>  0.0209
      2        <span class="ansi-cyan-fg">0.6905</span>       <span class="ansi-green-fg">0.6343</span>        <span class="ansi-magenta-fg">0.6979</span>  0.0208
      3        <span class="ansi-cyan-fg">0.6903</span>       0.6343        <span class="ansi-magenta-fg">0.6957</span>  0.0239
      4        <span class="ansi-cyan-fg">0.6897</span>       <span class="ansi-green-fg">0.6418</span>        <span class="ansi-magenta-fg">0.6938</span>  0.0241
      5        <span class="ansi-cyan-fg">0.6833</span>       0.6418        <span class="ansi-magenta-fg">0.6924</span>  0.0204
      6        <span class="ansi-cyan-fg">0.6768</span>       0.6418        <span class="ansi-magenta-fg">0.6915</span>  0.0154
      7        0.6855       <span class="ansi-green-fg">0.6493</span>        <span class="ansi-magenta-fg">0.6902</span>  0.0238
      8        0.6846       0.6343        <span class="ansi-magenta-fg">0.6888</span>  0.0179
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      9        <span class="ansi-cyan-fg">0.6766</span>       0.6418        <span class="ansi-magenta-fg">0.6876</span>  0.0214
     10        0.6770       0.6418        <span class="ansi-magenta-fg">0.6862</span>  0.0187
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7033</span>       <span class="ansi-green-fg">0.5821</span>        <span class="ansi-magenta-fg">0.6756</span>  0.0210
      2        <span class="ansi-cyan-fg">0.6913</span>       <span class="ansi-green-fg">0.5896</span>        <span class="ansi-magenta-fg">0.6745</span>  0.0168
      3        <span class="ansi-cyan-fg">0.6855</span>       0.5896        <span class="ansi-magenta-fg">0.6730</span>  0.0200
      4        <span class="ansi-cyan-fg">0.6782</span>       <span class="ansi-green-fg">0.6045</span>        <span class="ansi-magenta-fg">0.6717</span>  0.0222
      5        0.6817       0.6045        <span class="ansi-magenta-fg">0.6706</span>  0.0243
      6        0.6796       <span class="ansi-green-fg">0.6194</span>        <span class="ansi-magenta-fg">0.6689</span>  0.0228
      7        0.6827       <span class="ansi-green-fg">0.6343</span>        <span class="ansi-magenta-fg">0.6677</span>  0.0178
      8        0.6838       0.6269        <span class="ansi-magenta-fg">0.6660</span>  0.0235
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      9        <span class="ansi-cyan-fg">0.6778</span>       <span class="ansi-green-fg">0.6418</span>        <span class="ansi-magenta-fg">0.6645</span>  0.0253
     10        <span class="ansi-cyan-fg">0.6717</span>       <span class="ansi-green-fg">0.6567</span>        <span class="ansi-magenta-fg">0.6629</span>  0.0195
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7183</span>       <span class="ansi-green-fg">0.4179</span>        <span class="ansi-magenta-fg">0.7258</span>  0.0165
      2        0.7233       0.4179        <span class="ansi-magenta-fg">0.7195</span>  0.0204
      3        <span class="ansi-cyan-fg">0.7131</span>       0.4104        <span class="ansi-magenta-fg">0.7143</span>  0.0184
      4        <span class="ansi-cyan-fg">0.6996</span>       0.4179        <span class="ansi-magenta-fg">0.7108</span>  0.0180
      5        0.7048       <span class="ansi-green-fg">0.4403</span>        <span class="ansi-magenta-fg">0.7078</span>  0.0231
      6        <span class="ansi-cyan-fg">0.6979</span>       0.4403        <span class="ansi-magenta-fg">0.7051</span>  0.0170
      7        <span class="ansi-cyan-fg">0.6907</span>       <span class="ansi-green-fg">0.4627</span>        <span class="ansi-magenta-fg">0.7031</span>  0.0204
      8        0.6979       <span class="ansi-green-fg">0.4851</span>        <span class="ansi-magenta-fg">0.7012</span>  0.0168
      9        <span class="ansi-cyan-fg">0.6811</span>       <span class="ansi-green-fg">0.5149</span>        <span class="ansi-magenta-fg">0.6989</span>  0.0193
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>     10        0.6941       0.5149        <span class="ansi-magenta-fg">0.6968</span>  0.0195
     11        0.6922       <span class="ansi-green-fg">0.5299</span>        <span class="ansi-magenta-fg">0.6951</span>  0.0223
     12        <span class="ansi-cyan-fg">0.6811</span>       <span class="ansi-green-fg">0.5746</span>        <span class="ansi-magenta-fg">0.6931</span>  0.0162
     13        0.6874       <span class="ansi-green-fg">0.5821</span>        <span class="ansi-magenta-fg">0.6911</span>  0.0212
     14        <span class="ansi-cyan-fg">0.6775</span>       0.5746        <span class="ansi-magenta-fg">0.6899</span>  0.0220
     15        0.6784       <span class="ansi-green-fg">0.5896</span>        <span class="ansi-magenta-fg">0.6881</span>  0.0206
     16        <span class="ansi-cyan-fg">0.6737</span>       0.5821        <span class="ansi-magenta-fg">0.6862</span>  0.0156
     17        0.6810       0.5672        <span class="ansi-magenta-fg">0.6843</span>  0.0194
     18        <span class="ansi-cyan-fg">0.6712</span>       0.5672        <span class="ansi-magenta-fg">0.6830</span>  0.0203
     19        0.6739       0.5746        <span class="ansi-magenta-fg">0.6812</span>  0.0227
     20        0.6751       0.5597        <span class="ansi-magenta-fg">0.6797</span>  0.0205
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.6873</span>       <span class="ansi-green-fg">0.6343</span>        <span class="ansi-magenta-fg">0.6672</span>  0.0152
      2        <span class="ansi-cyan-fg">0.6844</span>       0.6269        <span class="ansi-magenta-fg">0.6658</span>  0.0168
      3        <span class="ansi-cyan-fg">0.6766</span>       0.6269        <span class="ansi-magenta-fg">0.6638</span>  0.0226
      4        <span class="ansi-cyan-fg">0.6737</span>       0.6343        <span class="ansi-magenta-fg">0.6625</span>  0.0188
      5        0.6804       0.6269        <span class="ansi-magenta-fg">0.6612</span>  0.0212
      6        0.6781       0.6269        <span class="ansi-magenta-fg">0.6599</span>  0.0154
      7        <span class="ansi-cyan-fg">0.6717</span>       0.6343        <span class="ansi-magenta-fg">0.6592</span>  0.0280
      8        0.6745       0.6269        <span class="ansi-magenta-fg">0.6575</span>  0.0191
      9        <span class="ansi-cyan-fg">0.6655</span>       0.6343        <span class="ansi-magenta-fg">0.6560</span>  0.0162
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>     10        <span class="ansi-cyan-fg">0.6600</span>       0.6343        <span class="ansi-magenta-fg">0.6545</span>  0.0256
     11        0.6663       0.6269        <span class="ansi-magenta-fg">0.6533</span>  0.0157
     12        0.6631       0.6343        <span class="ansi-magenta-fg">0.6532</span>  0.0194
     13        0.6603       0.6269        <span class="ansi-magenta-fg">0.6518</span>  0.0225
     14        0.6607       0.6269        <span class="ansi-magenta-fg">0.6511</span>  0.0139
     15        <span class="ansi-cyan-fg">0.6564</span>       0.6269        <span class="ansi-magenta-fg">0.6497</span>  0.0176
     16        0.6594       0.6343        <span class="ansi-magenta-fg">0.6485</span>  0.0164
     17        0.6615       <span class="ansi-green-fg">0.6418</span>        <span class="ansi-magenta-fg">0.6471</span>  0.0192
     18        <span class="ansi-cyan-fg">0.6546</span>       0.6343        <span class="ansi-magenta-fg">0.6467</span>  0.0178
     19        <span class="ansi-cyan-fg">0.6504</span>       0.6343        <span class="ansi-magenta-fg">0.6454</span>  0.0169
     20        <span class="ansi-cyan-fg">0.6495</span>       0.6343        <span class="ansi-magenta-fg">0.6437</span>  0.0192
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7136</span>       <span class="ansi-green-fg">0.5149</span>        <span class="ansi-magenta-fg">0.7052</span>  0.0150
      2        <span class="ansi-cyan-fg">0.7037</span>       <span class="ansi-green-fg">0.5299</span>        <span class="ansi-magenta-fg">0.7031</span>  0.0259
      3        <span class="ansi-cyan-fg">0.7030</span>       0.5299        <span class="ansi-magenta-fg">0.7018</span>  0.0214
      4        <span class="ansi-cyan-fg">0.6976</span>       <span class="ansi-green-fg">0.5448</span>        <span class="ansi-magenta-fg">0.7009</span>  0.0215
      5        0.7001       0.5448        <span class="ansi-magenta-fg">0.6997</span>  0.0205
      6        <span class="ansi-cyan-fg">0.6933</span>       0.5000        <span class="ansi-magenta-fg">0.6988</span>  0.0188
      7        0.7000       0.5075        <span class="ansi-magenta-fg">0.6975</span>  0.0206
      8        0.6944       0.4925        <span class="ansi-magenta-fg">0.6959</span>  0.0329
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      9        <span class="ansi-cyan-fg">0.6818</span>       0.5373        <span class="ansi-magenta-fg">0.6944</span>  0.0196
     10        <span class="ansi-cyan-fg">0.6808</span>       0.5299        <span class="ansi-magenta-fg">0.6932</span>  0.0187
     11        0.6829       0.5075        <span class="ansi-magenta-fg">0.6921</span>  0.0175
     12        <span class="ansi-cyan-fg">0.6754</span>       0.5149        <span class="ansi-magenta-fg">0.6912</span>  0.0192
     13        0.6806       0.5299        <span class="ansi-magenta-fg">0.6905</span>  0.0203
     14        <span class="ansi-cyan-fg">0.6741</span>       0.5299        <span class="ansi-magenta-fg">0.6896</span>  0.0222
     15        <span class="ansi-cyan-fg">0.6689</span>       0.5299        <span class="ansi-magenta-fg">0.6894</span>  0.0199
     16        0.6787       0.5299        <span class="ansi-magenta-fg">0.6883</span>  0.0206
     17        0.6735       0.5299        <span class="ansi-magenta-fg">0.6869</span>  0.0152
     18        <span class="ansi-cyan-fg">0.6669</span>       0.5373        <span class="ansi-magenta-fg">0.6857</span>  0.0208
     19        <span class="ansi-cyan-fg">0.6634</span>       0.5373        <span class="ansi-magenta-fg">0.6850</span>  0.0151
     20        0.6738       0.5373        <span class="ansi-magenta-fg">0.6839</span>  0.0197
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7007</span>       <span class="ansi-green-fg">0.5000</span>        <span class="ansi-magenta-fg">0.7011</span>  0.0154
      2        <span class="ansi-cyan-fg">0.6990</span>       0.5000        <span class="ansi-magenta-fg">0.6993</span>  0.0221
      3        <span class="ansi-cyan-fg">0.6969</span>       0.5000        <span class="ansi-magenta-fg">0.6981</span>  0.0210
      4        0.6977       0.5000        <span class="ansi-magenta-fg">0.6967</span>  0.0216
      5        <span class="ansi-cyan-fg">0.6909</span>       0.5000        <span class="ansi-magenta-fg">0.6944</span>  0.0155
      6        <span class="ansi-cyan-fg">0.6873</span>       0.5000        <span class="ansi-magenta-fg">0.6922</span>  0.0223
      7        0.6895       0.5000        <span class="ansi-magenta-fg">0.6903</span>  0.0187
      8        <span class="ansi-cyan-fg">0.6834</span>       <span class="ansi-green-fg">0.5075</span>        <span class="ansi-magenta-fg">0.6880</span>  0.0162
      9        0.6888       0.5075        <span class="ansi-magenta-fg">0.6866</span>  0.0233
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>     10        0.6857       0.5075        <span class="ansi-magenta-fg">0.6847</span>  0.0200
     11        <span class="ansi-cyan-fg">0.6767</span>       0.5075        <span class="ansi-magenta-fg">0.6830</span>  0.0253
     12        0.6819       <span class="ansi-green-fg">0.5224</span>        <span class="ansi-magenta-fg">0.6815</span>  0.0153
     13        0.6861       <span class="ansi-green-fg">0.5299</span>        <span class="ansi-magenta-fg">0.6803</span>  0.0268
     14        0.6801       <span class="ansi-green-fg">0.5821</span>        <span class="ansi-magenta-fg">0.6778</span>  0.0246
     15        <span class="ansi-cyan-fg">0.6759</span>       0.5821        <span class="ansi-magenta-fg">0.6757</span>  0.0167
     16        0.6831       0.5821        <span class="ansi-magenta-fg">0.6740</span>  0.0182
     17        <span class="ansi-cyan-fg">0.6727</span>       0.5821        <span class="ansi-magenta-fg">0.6725</span>  0.0154
     18        0.6749       <span class="ansi-green-fg">0.5896</span>        <span class="ansi-magenta-fg">0.6704</span>  0.0188
     19        <span class="ansi-cyan-fg">0.6718</span>       0.5896        <span class="ansi-magenta-fg">0.6687</span>  0.0239
     20        0.6724       <span class="ansi-green-fg">0.5970</span>        <span class="ansi-magenta-fg">0.6668</span>  0.0232
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7003</span>       <span class="ansi-green-fg">0.4851</span>        <span class="ansi-magenta-fg">0.6998</span>  0.0182
      2        <span class="ansi-cyan-fg">0.6949</span>       <span class="ansi-green-fg">0.5075</span>        <span class="ansi-magenta-fg">0.6977</span>  0.0192
      3        <span class="ansi-cyan-fg">0.6894</span>       <span class="ansi-green-fg">0.5224</span>        <span class="ansi-magenta-fg">0.6965</span>  0.0225
      4        0.6924       <span class="ansi-green-fg">0.5373</span>        <span class="ansi-magenta-fg">0.6952</span>  0.0241
      5        <span class="ansi-cyan-fg">0.6788</span>       <span class="ansi-green-fg">0.5448</span>        <span class="ansi-magenta-fg">0.6937</span>  0.0224
      6        0.6921       <span class="ansi-green-fg">0.5522</span>        <span class="ansi-magenta-fg">0.6916</span>  0.0240
      7        0.6931       <span class="ansi-green-fg">0.5672</span>        <span class="ansi-magenta-fg">0.6910</span>  0.0199
      8        <span class="ansi-cyan-fg">0.6787</span>       0.5672        <span class="ansi-magenta-fg">0.6895</span>  0.0245
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>      9        0.6837       0.5597        <span class="ansi-magenta-fg">0.6880</span>  0.0170
     10        0.6795       0.5672        <span class="ansi-magenta-fg">0.6864</span>  0.0226
     11        <span class="ansi-cyan-fg">0.6764</span>       <span class="ansi-green-fg">0.5896</span>        <span class="ansi-magenta-fg">0.6846</span>  0.0190
     12        0.6784       0.5896        <span class="ansi-magenta-fg">0.6837</span>  0.0232
     13        <span class="ansi-cyan-fg">0.6736</span>       0.5896        <span class="ansi-magenta-fg">0.6825</span>  0.0210
     14        <span class="ansi-cyan-fg">0.6673</span>       0.5896        <span class="ansi-magenta-fg">0.6812</span>  0.0187
     15        <span class="ansi-cyan-fg">0.6622</span>       <span class="ansi-green-fg">0.6119</span>        <span class="ansi-magenta-fg">0.6796</span>  0.0226
     16        0.6638       <span class="ansi-green-fg">0.6343</span>        <span class="ansi-magenta-fg">0.6777</span>  0.0152
     17        0.6644       0.6269        <span class="ansi-magenta-fg">0.6769</span>  0.0158
     18        0.6635       <span class="ansi-green-fg">0.6493</span>        <span class="ansi-magenta-fg">0.6751</span>  0.0177
     19        <span class="ansi-cyan-fg">0.6594</span>       <span class="ansi-green-fg">0.6642</span>        <span class="ansi-magenta-fg">0.6737</span>  0.0152
     20        <span class="ansi-cyan-fg">0.6532</span>       0.6642        <span class="ansi-magenta-fg">0.6717</span>  0.0227
  epoch    train_loss    valid_acc    valid_loss     dur
-------  ------------  -----------  ------------  ------
      1        <span class="ansi-cyan-fg">0.7076</span>       <span class="ansi-green-fg">0.5000</span>        <span class="ansi-magenta-fg">0.7047</span>  0.0150
      2        <span class="ansi-cyan-fg">0.6998</span>       0.5000        <span class="ansi-magenta-fg">0.7036</span>  0.0209
      3        <span class="ansi-cyan-fg">0.6988</span>       0.5000        <span class="ansi-magenta-fg">0.7025</span>  0.0212
      4        <span class="ansi-cyan-fg">0.6962</span>       <span class="ansi-green-fg">0.5075</span>        <span class="ansi-magenta-fg">0.7012</span>  0.0194
      5        <span class="ansi-cyan-fg">0.6891</span>       <span class="ansi-green-fg">0.5149</span>        <span class="ansi-magenta-fg">0.7000</span>  0.0160
      6        <span class="ansi-cyan-fg">0.6862</span>       0.5075        <span class="ansi-magenta-fg">0.6990</span>  0.0167
      7        <span class="ansi-cyan-fg">0.6826</span>       0.4925        <span class="ansi-magenta-fg">0.6977</span>  0.0225
      8        <span class="ansi-cyan-fg">0.6811</span>       0.4701        <span class="ansi-magenta-fg">0.6969</span>  0.0184
      9        <span class="ansi-cyan-fg">0.6789</span>       0.4701        <span class="ansi-magenta-fg">0.6961</span>  0.0261
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>&lt;ipython-input-4-467d7b16a0d1&gt;:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  X = F.softmax(self.output(X))
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>     10        0.6801       0.5075        <span class="ansi-magenta-fg">0.6952</span>  0.0242
     11        0.6804       0.5075        <span class="ansi-magenta-fg">0.6946</span>  0.0165
     12        <span class="ansi-cyan-fg">0.6761</span>       0.5075        <span class="ansi-magenta-fg">0.6940</span>  0.0215
     13        0.6779       0.5149        <span class="ansi-magenta-fg">0.6932</span>  0.0151
     14        <span class="ansi-cyan-fg">0.6695</span>       <span class="ansi-green-fg">0.5448</span>        <span class="ansi-magenta-fg">0.6925</span>  0.0208
     15        0.6747       <span class="ansi-green-fg">0.5672</span>        <span class="ansi-magenta-fg">0.6915</span>  0.0228
     16        <span class="ansi-cyan-fg">0.6692</span>       0.5522        <span class="ansi-magenta-fg">0.6908</span>  0.0216
     17        0.6760       0.5448        <span class="ansi-magenta-fg">0.6897</span>  0.0222
     18        <span class="ansi-cyan-fg">0.6680</span>       0.5448        <span class="ansi-magenta-fg">0.6888</span>  0.0150
     19        0.6689       0.5672        <span class="ansi-magenta-fg">0.6881</span>  0.0221
     20        <span class="ansi-cyan-fg">0.6641</span>       0.5672        <span class="ansi-magenta-fg">0.6874</span>  0.0145
0.6680392967818117 {&#39;lr&#39;: 0.02, &#39;max_epochs&#39;: 10, &#39;module__num_units&#39;: 20}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

